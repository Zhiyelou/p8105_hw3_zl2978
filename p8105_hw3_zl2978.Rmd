---
title: "p8105_hw3_zl2978"
author: Zhiye Lou
output: github_document
---
```{r, include=FALSE}
library(tidyverse)
library(patchwork)
knitr::opts_chunk$set(
	fig.width = 16, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
##Problem 1  
I will first load the data.
```{r}
library(p8105.datasets)
data("instacart")
```
This dataset instacart contains `r nrow("instacart")` rows and `r ncol("instacart")` columns. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes.
Then I will do some exploration of this dataset.
```{r}
aisleinfo_df =
  instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```
There are `r nrow(aisleinfo_df)` aisles, and the most items ordered from is  `r pull(aisleinfo_df %>% head(1),aisle)`.

Then I will make a plot
```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```
Then I will make a table 
```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```
Then I will make a table showing the mean hour of "Apples" vs "Ice cream"
```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	) %>% 
  rename( 
      "Sunday" = "0",
      "Monday" = "1",
      "Tuesday" = "2",
      "Wednesday" = "3",
      "Thursday" = "4",
      "Friday" = "5",
      "Saturday" = "6") %>% 
  knitr::kable()
```

##Problem 2  
###question 1   
I will first load and tidy the data.
```{r}
week_nam = tibble(
  n = 0:6,
  day = c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))
accel_df = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>%
  left_join(week_nam, by = "day") %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes_of_the_day",
    values_to = "activity_count") %>% 
  mutate(
    minutes_of_the_day = substr(minutes_of_the_day,10,13),
    weekday_weekend = if_else(day %in% c("Saturday","Sunday"),"Weekend","Weekday"),
    minutes_of_the_day = as.numeric(minutes_of_the_day),
    day = factor(day),
    day = fct_reorder(day,n))%>% 
  rename("day_of_the_week" = "day") %>% 
  group_by(week) %>% 
  arrange(day_of_the_week, desc(n), .by_group = TRUE) %>% 
  select(-n) %>% 
  relocate(week,day_id,day_of_the_week,weekday_weekend)
```
There are `r nrow(accel_df)` observations in total, and there are `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. There are information about the time of recorded variables -- week, day_id, day_of_the_week,weekday_weekend, minutes_of_the_day. The other important variable is the activity_count which is the activity counts.

###question 2  
I will create a table and a plot showing the total activity for each day.
```{r}
acc_total_df =
   accel_df %>% 
   group_by(week,day_of_the_week,day_id) %>% 
   summarise(total_of_the_day = sum(activity_count))
day_of_week_p = 
  ggplot(data = acc_total_df,aes(x = day_id,y = total_of_the_day, color = day_of_the_week)) + 
  geom_point() + 
  geom_line()
acc_total_df %>% 
  select(-day_id) %>% 
  pivot_wider(names_from = week,
              names_prefix = "week ",
              values_from = total_of_the_day) %>%
  knitr::kable()
```
I will create a plot showing the mean activity for weekdays and weekends.
```{r}
weekday_vs_weekend_p = 
  accel_df %>% 
  group_by(week,weekday_weekend) %>% 
  summarise(mean_of_the_weekday_end = mean(activity_count)) %>% 
  ggplot(aes(x = week,y = mean_of_the_weekday_end, color = weekday_weekend)) + 
  geom_point() + 
  geom_line()
```
I will create a plot showing the trend of different weeks.
```{r}
week_trend_p = 
  accel_df %>% 
  group_by(week) %>% 
  summarise(total_of_week = sum(activity_count)) %>% 
ggplot(aes(x = week,y = total_of_week)) + 
geom_point() + geom_line()
```
Then I will join three graphs together
```{r}
(week_trend_p + weekday_vs_weekend_p)/day_of_week_p
```

First, the difference between weekdays and weekends are apparent. For the first two weeks, the mean of activity counts for weekends are greater than that of weekdays. for the rest three weeks, the mean of the weekends has largely decreased and be smaller than the mean of weekdays.
Second, the activity counts have increased from week 1 to week 2, and largely decreased from week 3 to week 4, and there is a smaller increase from week 4 to week 5.
Third, the difference between different days of a week is not apparent, but Tuesday, Wednesday,and Thursday have relatively smaller changes in activity counts among different weeks.   


###question 3 
I will make a single-panel plot that shows the 24-hour activity time courses for each day.
```{r}
accel_df %>% 
  ggplot(aes(x = minutes_of_the_day, y = activity_count,color = day_of_the_week)) +   
  geom_line(alpha = 0.2) +
  stat_smooth(se = FALSE)
```
Without further zoom in,the trend in different minutes of each day is not that obvious, and thus I zoom in with the limit of activity counts between 0 to 2000.
```{r}
accel_df %>% 
  ggplot(aes(x = minutes_of_the_day, y = activity_count,color = day_of_the_week)) +  
  geom_line(alpha = 0.2) + 
  stat_smooth(se = FALSE) + 
  coord_cartesian(ylim = c(0,2000))
```
This plot shows that between 0 to 250 minutes (around 0:00 to 4:10), the activity counts are pretty low. After 250 minutes, the activity counts start to increase which implies there are more activities at day time. Then, around 1250 minutes (around 20:50), the activity counts start to decrease and this implies there are less activities at evening and night time. Finally, the activity counts get close to 0 around 1440 minutes (around 24:00), and this suggests there is almost no activity at mid-night.

##Problem 3
I will frist load the data.
```{r}
library(p8105.datasets)
data("ny_noaa")
```

###question 1  
For this data set, there are `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. This datasets has the weather station id -- id. There is also a variable date, and weather variables -- prcp (precipitation), snow, snwd (snow depth), tmax(maximum temperature), and tmin(minimum temperature). There are `r nrow(ny_noaa[complete.cases(ny_noaa),])` rows that do not have missing values, and the reasons for these missing values are unknown, and thus these missing value could be problematic.

Then I will do some data cleaning.
```{r}
nyweather_df =
  ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year","month","day"), sep = "-") %>% 
  mutate(
    tmax = as.numeric(tmax)/10,
    tmin = as.numeric(tmin)/10,
    prcp = prcp/10) %>% 
  rename(
    "tmax_C" = "tmax",
    "tmin_C" = "tmin") 
snow_count_df =
  nyweather_df %>% 
  count(snow) %>% 
	arrange(desc(n)) 
```
The most commonly observed values are 0, because for most of the days, there are no snow, and thus the snowfall is 0 mm. The second frequently value for snow is NA which suggests a large portion of missing values. The third and fourth frequently value for snow is 25 and 13 respectively, and this suggests a moderate snow is very frequently when there is a snow.

##question 2
Then I will make a two-panel plot showing the average max temperature in January and in July.
```{r}
tmax_jj_df = 
nyweather_df %>% 
  filter(month %in% c("01","07")) %>% 
  mutate(month = if_else (month == "01","January","July")) %>% 
  group_by(month,id,year) %>% 
  summarise(mean_tmax = mean(tmax_C)) 
ggplot(data = tmax_jj_df, aes(x = year, y = mean_tmax ,color = id, group = id)) + 
geom_point(alpha = 0.3) +
geom_path(alpha = 0.2) + 
facet_grid(.~month) + 
theme(legend.position = "none", axis.title.x = element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + 
labs(y = "mean of maximum temperature")
```
The most obvious trend is the difference between average maximum temperature between January and July. Almost every year and every station in July has a higher average maximum temperature than that of January. Besides,the outliers in July are more obvious, and they tend to be the outliers that much lower than other values. To be more accurate about that, I will plot a boxplot.
```{r}
ggplot(data = tmax_jj_df, aes(x = year, y = mean_tmax ,color = month)) +
geom_boxplot() +
facet_grid(.~month) + 
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
labs(y = "mean of maximum temperature")
```
As the boxplot shown, the outliers for January are closer to the range of their regular values compare to those of July, and for January, the outliers are tend to be higher than the range of regular values.

##question 3
I will first make a plot for tmax vs tmin for the full dataset.
```{r}
min_max_p = 
nyweather_df %>% 
ggplot(aes(x = tmin_C, y = tmax_C)) +
stat_bin2d(bins = 50) +
theme(legend.position = "right")
```
Then I will plot the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r}
snow_regular_p =
nyweather_df %>% 
  filter(snow > 0) %>% 
  filter(snow < 100) %>% 
ggplot(aes(x = year, y = snow)) +
geom_boxplot() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
min_max_p/snow_regular_p + 
ggsave("P_3_2.pdf",width = 10, height = 15)
```
The bin2d plots of maximum temperature vs mimimum temperature shown the maximum and minimum temperatures are concentrated at (0,7) and (17,25), and the gradient of this plot is close to 1, and this suggest that an increasing in minimum temperature usually comes with an increasing in maximum temperature. The distribution of snow fall graph shown the variability of regular snow fall (between 0 and 100) is pretty small, and only 1998,2006, and 2010 has relatively smaller snow fall than other years', and have several outliers. For other years, there are nearly no outliers, and the medium, 25 quantile, 75 quantile are nearly the same. Besides, except 2006, all the other years' snow fall data are right skewed.
 






